{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8f1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from tensorflow.keras import activations, backend\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Model, Input\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import concatenate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn import over_sampling\n",
    "import random\n",
    "from numpy.random import seed\n",
    "from sklearn.calibration import calibration_curve\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e88655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the proposed model\n",
    "seed(100)\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def build_model(lstm_unit):\n",
    "    time_input = keras.Input(shape=(200,94), name='time_input')\n",
    "    time_step_input=keras.Input(shape=(200,1), name='time_step_input')\n",
    "    time_step_output=layers.Dense(1,use_bias=True)(time_step_input/180)\n",
    "    time_step_output=1-tf.keras.activations.tanh(tf.math.pow(time_step_output,2))\n",
    "    time_output = layers.Attention()([time_input,time_input])\n",
    "    time_output=concatenate([time_input,time_step_output],axis=-1)\n",
    "    lstm_output2 = layers.LSTM(lstm_unit)(time_output)\n",
    "    lstm_output2=keras.backend.expand_dims(lstm_output2)\n",
    "    lstm_output_time, weights1 = layers.MultiHeadAttention(num_heads=2, key_dim=2,attention_axes=(1))(lstm_output2, time_step_output,return_attention_scores=True)\n",
    "    lstm_output_time=keras.backend.squeeze(lstm_output_time,axis=-1)\n",
    "    lstm_output = layers.Dense(32, activation=\"relu\")(lstm_output_time)\n",
    "    lstm_output = Dropout(0.2)(lstm_output)\n",
    "    preds = layers.Dense(1, activation=\"sigmoid\")(lstm_output)\n",
    "    model = Model(inputs=[time_input,time_step_input], outputs=preds)\n",
    "    nadam = keras.optimizers.Nadam()\n",
    "    model.compile(optimizer=nadam, loss='binary_crossentropy', metrics=['accuracy',recall, precision])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e54d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training, calibration and prediction\n",
    "def model_performance(modelname,task,epoch,unit):\n",
    "    global traindata\n",
    "    global predictiontask\n",
    "    global trainsize\n",
    "    global testsize\n",
    "    global calibrationsize\n",
    "    if task=='onset6':\n",
    "        traindata=labdata_onset6\n",
    "    if task=='onset12':\n",
    "        traindata=labdata_onset12\n",
    "    if task=='onset24':\n",
    "        traindata=labdata_onset24\n",
    "    if task=='onset48':\n",
    "        traindata=labdata_onset48\n",
    "    if task=='onset72':\n",
    "        traindata=labdata_onset72\n",
    "    if task=='onset168':\n",
    "        traindata=labdata_onset168\n",
    "    if task=='surgery0':\n",
    "        traindata=labdata_surgery0\n",
    "    if task=='surgery6':\n",
    "        traindata=labdata_surgery6\n",
    "    if task=='surgery12':\n",
    "        traindata=labdata_surgery12\n",
    "    if task=='surgery24':\n",
    "        traindata=labdata_surgery24\n",
    "    if task=='surgery48':\n",
    "        traindata=labdata_surgery48\n",
    "    if task=='surgery72':\n",
    "        traindata=labdata_surgery72\n",
    "    if task=='random6' or task=='random12' or task=='random24' or task=='random48' or task=='random72' or task=='random168':\n",
    "        traindata=labdata_random\n",
    "    if task=='onset6' or task=='onset12' or task=='onset24' or task=='onset48' or task=='onset72' or task=='onset168':\n",
    "        predictiontask='anyAKI'\n",
    "    if task=='surgery0' or task=='surgery6' or task=='surgery12' or task=='surgery24' or task=='surgery48' or task=='surgery72':\n",
    "        predictiontask='anyAKI'\n",
    "    if task=='random6':\n",
    "        predictiontask='aki0_6'\n",
    "    if task=='random12':\n",
    "        predictiontask='aki0_12'\n",
    "    if task=='random24':\n",
    "        predictiontask='aki0_24'\n",
    "    if task=='random48':\n",
    "        predictiontask='aki0_48'\n",
    "    if task=='random72':\n",
    "        predictiontask='aki0_72'\n",
    "    if task=='random168':\n",
    "        predictiontask='aki0_168'\n",
    "    a=CHDdata_AKI_prediction.drop(['aki0_6', 'aki0_12', 'aki0_24', 'aki0_48',\n",
    "       'aki0_72', 'aki0_168','anyAKI'],axis=1)\n",
    "    b=CHDdata_AKI_prediction[[predictiontask]]\n",
    "    a_train, a_testp, b_train, b_testp = train_test_split(a, b, test_size=0.4,random_state=150)\n",
    "    ros = over_sampling.RandomOverSampler(random_state=1)\n",
    "    a_train_ros, b_train_ros = ros.fit_resample(a_train, b_train)\n",
    "    a_test_ros, b_test_ros=ros.fit_resample(a_testp, b_testp)\n",
    "    list1=a_train_ros.index.to_list()\n",
    "    random.shuffle(list1)\n",
    "    a_train_ros1=a_train_ros.loc[list1,:]\n",
    "    b_train_ros1=b_train_ros.loc[list1,:]\n",
    "    list2=a_test_ros.index.to_list()\n",
    "    random.shuffle(list2)\n",
    "    a_test_ros1=a_test_ros.loc[list2,:]\n",
    "    b_test_ros1=b_test_ros.loc[list2,:]\n",
    "    trainsize=a_train_ros1.shape[0]    \n",
    "    j=0\n",
    "    a_train1=np.zeros((trainsize,200,94))\n",
    "    a_train2=np.zeros((trainsize,200,1))\n",
    "    basefile='G:/AKIprediction/SHAP_SMOTE_result/'\n",
    "    for i in a_train_ros1.index.to_list():\n",
    "        Id=a_train_ros1.loc[i,'ID']\n",
    "        a_train1[j,:80,:8]=vital_sign_data[Id,:1,:]\n",
    "        a_train1[j,180:,:8]=vital_sign_data[Id,99:,:]\n",
    "        a_train1[j,80:180,:8]=vital_sign_data[Id,:,:]\n",
    "        a_train2[j,:80,:]=traindata[Id,:80,42:]\n",
    "        a_train2[j,180:,:]=traindata[Id,80:,42:]\n",
    "        a_train2[j,80:130,:]=traindata[Id,80,42:]+1\n",
    "        a_train2[j,130:180,:]=traindata[Id,80,42:]+2\n",
    "        a_train1[j,:80,8:50]=traindata[Id,:80,:42]\n",
    "        a_train1[j,180:,8:50]=traindata[Id,80:,:42]\n",
    "        a_train1[j,80:180,8:50]=traindata[Id,79:80,:42]\n",
    "        a_train1[j,:,50:]=np.tile(np.array(a_train_ros1.iloc[j,:44]),(200,1))\n",
    "        j+=1\n",
    "    model=build_model(unit)\n",
    "    model.fit({'time_input':a_train1,'time_step_input':a_train2},b_train_ros1, epochs=epoch,batch_size=64,\n",
    "          validation_split=0.15,verbose=2,validation_batch_size=None)\n",
    "    ourmodelresult=np.zeros((100,13))\n",
    "    tprs=[]\n",
    "    precisions=[]\n",
    "    prob_trues=[]\n",
    "    re_prob_trues=[]\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    mean_recall = np.linspace(0,1,100)\n",
    "    mean_prob_pred=np.linspace(0,1,20)\n",
    "    re_mean_prob_pred=np.linspace(0,1,20)\n",
    "    for s in range(100):\n",
    "        a_calibration, a_test, b_calibration, b_test = train_test_split(a_test_ros1, b_test_ros1, test_size=0.6)\n",
    "        calibrationsize=a_calibration.shape[0]\n",
    "        testsize=a_test.shape[0]\n",
    "        j=0\n",
    "        a_calibration1=np.zeros((calibrationsize,200,94))\n",
    "        a_calibration2=np.zeros((calibrationsize,200,1))\n",
    "        for i in a_calibration.index.to_list():\n",
    "            Id=a_calibration.loc[i,'ID']\n",
    "            a_calibration1[j,:80,:8]=vital_sign_data[Id,:1,:]\n",
    "            a_calibration1[j,180:,:8]=vital_sign_data[Id,99:,:]\n",
    "            a_calibration1[j,80:180,:8]=vital_sign_data[Id,:,:]\n",
    "            a_calibration2[j,:80,:]=traindata[Id,:80,42:]\n",
    "            a_calibration2[j,180:,:]=traindata[Id,80:,42:]\n",
    "            a_calibration2[j,80:130,:]=traindata[Id,80,42:]+1\n",
    "            a_calibration2[j,130:180,:]=traindata[Id,80,42:]+2\n",
    "            a_calibration1[j,:80,8:50]=traindata[Id,:80,:42]\n",
    "            a_calibration1[j,180:,8:50]=traindata[Id,80:,:42]\n",
    "            a_calibration1[j,80:180,8:50]=traindata[Id,79:80,:42]\n",
    "            a_calibration1[j,:,50:]=np.tile(np.array(a_calibration.iloc[j,:44]),(200,1))\n",
    "            j+=1\n",
    "        j=0\n",
    "        a_test1=np.zeros((testsize,200,94))\n",
    "        a_test2=np.zeros((testsize,200,1))\n",
    "        for i in a_test.index.to_list():\n",
    "            Id=a_test.loc[i,'ID']\n",
    "            a_test1[j,:80,:8]=vital_sign_data[Id,:1,:]\n",
    "            a_test1[j,180:,:8]=vital_sign_data[Id,99:,:]\n",
    "            a_test1[j,80:180,:8]=vital_sign_data[Id,:,:]\n",
    "            a_test2[j,:80,:]=traindata[Id,:80,42:]\n",
    "            a_test2[j,180:,:]=traindata[Id,80:,42:]\n",
    "            a_test2[j,80:130,:]=traindata[Id,80,42:]+1\n",
    "            a_test2[j,130:180,:]=traindata[Id,80,42:]+2\n",
    "            a_test1[j,:80,8:50]=traindata[Id,:80,:42]\n",
    "            a_test1[j,180:,8:50]=traindata[Id,80:,:42]\n",
    "            a_test1[j,80:180,8:50]=traindata[Id,79:80,:42]\n",
    "            a_test1[j,:,50:]=np.tile(np.array(a_test.iloc[j,:44]),(200,1))\n",
    "            j+=1\n",
    "        yhat=model.predict([a_calibration1,a_calibration2])\n",
    "        prediction=pd.DataFrame(yhat)\n",
    "        lrc = LogisticRegression(random_state=0,class_weight='balanced')\n",
    "        lrc.fit(pd.DataFrame(prediction[0]),b_calibration)\n",
    "        ir=IsotonicRegression()\n",
    "        ir.fit(pd.DataFrame(prediction[0]),b_calibration[predictiontask])\n",
    "        yhattest=model.predict([a_test1,a_test2])\n",
    "        predictionother=pd.DataFrame(yhattest)\n",
    "        predictionother[1]=predictionother[0].apply(lambda x:1 if x>=0.5 else 0)\n",
    "        predictionother1=lrc.predict_proba(pd.DataFrame(predictionother[0]))\n",
    "        predictionother1=pd.DataFrame(predictionother1)\n",
    "        predictionother2=ir.predict(pd.DataFrame(predictionother[0]))\n",
    "        predictionother2=pd.DataFrame(predictionother2)\n",
    "        predictionother[2]=predictionother1[1]\n",
    "        predictionother[3]=predictionother1[1].apply(lambda x:1 if x>=0.5 else 0)\n",
    "        predictionother[4]=predictionother2[0]\n",
    "        predictionother[4].fillna(0,inplace=True)\n",
    "        predictionother[5]=predictionother2[0].apply(lambda x:1 if x>=0.5 else 0)\n",
    "        ourmodelresult[s,0]=metrics.accuracy_score(b_test,predictionother[5])\n",
    "        ourmodelresult[s,1]=metrics.recall_score(b_test,predictionother[5])\n",
    "        precision,recall,_ = metrics.precision_recall_curve(b_test,predictionother[4])\n",
    "        ourmodelresult[s,2]=metrics.auc(recall,precision)\n",
    "        ourmodelresult[s,3]=metrics.roc_auc_score(b_test,predictionother[4])\n",
    "        ourmodelresult[s,4]=metrics.roc_auc_score(b_test,predictionother[0])\n",
    "        ourmodelresult[s,5]=metrics.brier_score_loss(b_test,predictionother[0])\n",
    "        ourmodelresult[s,6]=metrics.mean_absolute_error(b_test,predictionother[0])\n",
    "        ourmodelresult[s,7]=metrics.roc_auc_score(b_test,predictionother[2])\n",
    "        ourmodelresult[s,8]=metrics.brier_score_loss(b_test,predictionother[2])\n",
    "        ourmodelresult[s,9]=metrics.mean_absolute_error(b_test,predictionother[2])\n",
    "        ourmodelresult[s,10]=metrics.roc_auc_score(b_test,predictionother[4])\n",
    "        ourmodelresult[s,11]=metrics.brier_score_loss(b_test,predictionother[4])\n",
    "        ourmodelresult[s,12]=metrics.mean_absolute_error(b_test,predictionother[4])\n",
    "        fpr, tpr, _ =metrics.roc_curve(b_test,predictionother[4])\n",
    "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        precision,recall,_ = metrics.precision_recall_curve(b_test,predictionother[4])\n",
    "        recall=recall[::-1]\n",
    "        precision=precision[::-1]\n",
    "        interp_precision = np.interp(mean_recall, recall, precision)\n",
    "        precisions.append(interp_precision)\n",
    "        prob_true, prob_pred = calibration_curve(b_test,predictionother[0], n_bins=20,normalize=True)\n",
    "        interp_prob_true = np.interp(mean_prob_pred, prob_pred, prob_true)\n",
    "        prob_trues.append(interp_prob_true)\n",
    "        re_prob_true, re_prob_pred = calibration_curve(b_test,predictionother[4], n_bins=20,normalize=True)\n",
    "        interp_re_prob_true = np.interp(re_mean_prob_pred, re_prob_pred, re_prob_true)\n",
    "        re_prob_trues.append(interp_re_prob_true)\n",
    "    mean_precision = np.mean(precisions, axis=0)\n",
    "    std_precision = np.std(precisions, axis=0)\n",
    "    precision_upper = np.minimum(mean_precision + std_precision, 1)\n",
    "    precision_lower = np.maximum(mean_precision - std_precision, 0)\n",
    "    pr_curve_onset6=[]\n",
    "    pr_curve_onset6.append(mean_recall)\n",
    "    pr_curve_onset6.append(mean_precision)\n",
    "    pr_curve_onset6.append(precision_upper)\n",
    "    pr_curve_onset6.append(precision_lower)\n",
    "    pr_curve_onset6=pd.DataFrame(pr_curve_onset6)\n",
    "    pr_curve_onset6=pr_curve_onset6.T\n",
    "    mean_prob_trues = np.mean(prob_trues, axis=0)\n",
    "    std_prob_trues = np.std(prob_trues, axis=0)\n",
    "    prob_true_upper = np.minimum(mean_prob_trues + std_prob_trues, 1)\n",
    "    prob_true_lower = np.maximum(mean_prob_trues - std_prob_trues, 0)\n",
    "    calibration_curve_onset6=[]\n",
    "    calibration_curve_onset6.append(mean_prob_pred)\n",
    "    calibration_curve_onset6.append(mean_prob_trues)\n",
    "    calibration_curve_onset6.append(prob_true_upper)\n",
    "    calibration_curve_onset6.append(prob_true_lower)\n",
    "    calibration_curve_onset6=pd.DataFrame(calibration_curve_onset6)\n",
    "    calibration_curve_onset6=calibration_curve_onset6.T\n",
    "    re_mean_prob_trues = np.mean(re_prob_trues, axis=0)\n",
    "    re_std_prob_trues = np.std(re_prob_trues, axis=0)\n",
    "    re_prob_true_upper = np.minimum(re_mean_prob_trues + re_std_prob_trues, 1)\n",
    "    re_prob_true_lower = np.maximum(re_mean_prob_trues - re_std_prob_trues, 0)\n",
    "    re_calibration_curve_onset6=[]\n",
    "    re_calibration_curve_onset6.append(re_mean_prob_pred)\n",
    "    re_calibration_curve_onset6.append(re_mean_prob_trues)\n",
    "    re_calibration_curve_onset6.append(re_prob_true_upper)\n",
    "    re_calibration_curve_onset6.append(re_prob_true_lower)\n",
    "    re_calibration_curve_onset6=pd.DataFrame(re_calibration_curve_onset6)\n",
    "    re_calibration_curve_onset6=re_calibration_curve_onset6.T\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    roc_curve_onset6=[]\n",
    "    roc_curve_onset6.append(mean_fpr)\n",
    "    roc_curve_onset6.append(mean_tpr)\n",
    "    roc_curve_onset6.append(tprs_upper)\n",
    "    roc_curve_onset6.append(tprs_lower)\n",
    "    roc_curve_onset6=pd.DataFrame(roc_curve_onset6)\n",
    "    roc_curve_onset6=roc_curve_onset6.T\n",
    "    performance_metric=np.zeros((13,2))\n",
    "    performance_metric=pd.DataFrame(performance_metric)\n",
    "    ci0=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,0]), scale=stats.sem(ourmodelresult[:,0]))\n",
    "    mean0=np.mean(ourmodelresult[:,0])\n",
    "    performance_metric.iloc[0,0]='accuracy'\n",
    "    performance_metric.iloc[0,1]=str(format(mean0,'.3f'))+' ['+str(format(ci0[0],'.3f'))+'-'+str(format(ci0[1],'.3f'))+']'\n",
    "    ci1=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,1]), scale=stats.sem(ourmodelresult[:,1]))\n",
    "    mean1=np.mean(ourmodelresult[:,1])\n",
    "    performance_metric.iloc[1,0]='recall'\n",
    "    performance_metric.iloc[1,1]=str(format(mean1,'.3f'))+' ['+str(format(ci1[0],'.3f'))+'-'+str(format(ci1[1],'.3f'))+']'\n",
    "    ci2=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,2]), scale=stats.sem(ourmodelresult[:,2]))\n",
    "    mean2=np.mean(ourmodelresult[:,2])\n",
    "    performance_metric.iloc[2,0]='PR AUC'\n",
    "    performance_metric.iloc[2,1]=str(format(mean2,'.3f'))+' ['+str(format(ci2[0],'.3f'))+'-'+str(format(ci2[1],'.3f'))+']'\n",
    "    ci3=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,3]), scale=stats.sem(ourmodelresult[:,3]))\n",
    "    mean3=np.mean(ourmodelresult[:,3])\n",
    "    performance_metric.iloc[3,0]='ROC AUC'\n",
    "    performance_metric.iloc[3,1]=str(format(mean3,'.3f'))+' ['+str(format(ci3[0],'.3f'))+'-'+str(format(ci3[1],'.3f'))+']'\n",
    "    ci4=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,4]), scale=stats.sem(ourmodelresult[:,4]))\n",
    "    mean4=np.mean(ourmodelresult[:,4])\n",
    "    performance_metric.iloc[4,0]='uncalibration AUC'\n",
    "    performance_metric.iloc[4,1]=str(format(mean4,'.3f'))+' ['+str(format(ci4[0],'.3f'))+'-'+str(format(ci4[1],'.3f'))+']'\n",
    "    ci5=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,5]), scale=stats.sem(ourmodelresult[:,5]))\n",
    "    mean5=np.mean(ourmodelresult[:,5])\n",
    "    performance_metric.iloc[5,0]='uncalibration brier'\n",
    "    performance_metric.iloc[5,1]=str(format(mean5,'.3f'))+' ['+str(format(ci5[0],'.3f'))+'-'+str(format(ci5[1],'.3f'))+']'\n",
    "    ci6=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,6]), scale=stats.sem(ourmodelresult[:,6]))\n",
    "    mean6=np.mean(ourmodelresult[:,6])\n",
    "    performance_metric.iloc[6,0]='uncalibration mae'\n",
    "    performance_metric.iloc[6,1]=str(format(mean6,'.3f'))+' ['+str(format(ci6[0],'.3f'))+'-'+str(format(ci6[1],'.3f'))+']'\n",
    "    ci7=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,7]), scale=stats.sem(ourmodelresult[:,7]))\n",
    "    mean7=np.mean(ourmodelresult[:,7])\n",
    "    performance_metric.iloc[7,0]='scaling AUC'\n",
    "    performance_metric.iloc[7,1]=str(format(mean7,'.3f'))+' ['+str(format(ci7[0],'.3f'))+'-'+str(format(ci7[1],'.3f'))+']'\n",
    "    ci8=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,8]), scale=stats.sem(ourmodelresult[:,8]))\n",
    "    mean8=np.mean(ourmodelresult[:,8])\n",
    "    performance_metric.iloc[8,0]='scaling brier'\n",
    "    performance_metric.iloc[8,1]=str(format(mean8,'.3f'))+' ['+str(format(ci8[0],'.3f'))+'-'+str(format(ci8[1],'.3f'))+']'\n",
    "    ci9=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,9]), scale=stats.sem(ourmodelresult[:,9]))\n",
    "    mean9=np.mean(ourmodelresult[:,9])\n",
    "    performance_metric.iloc[9,0]='scaling mae'\n",
    "    performance_metric.iloc[9,1]=str(format(mean9,'.3f'))+' ['+str(format(ci9[0],'.3f'))+'-'+str(format(ci9[1],'.3f'))+']'\n",
    "    ci10=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,10]), scale=stats.sem(ourmodelresult[:,10]))\n",
    "    mean10=np.mean(ourmodelresult[:,10])\n",
    "    performance_metric.iloc[10,0]='isotonic AUC'\n",
    "    performance_metric.iloc[10,1]=str(format(mean10,'.3f'))+' ['+str(format(ci10[0],'.3f'))+'-'+str(format(ci10[1],'.3f'))+']'\n",
    "    ci11=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,11]), scale=stats.sem(ourmodelresult[:,11]))\n",
    "    mean11=np.mean(ourmodelresult[:,11])\n",
    "    performance_metric.iloc[11,0]='isotonic brier'\n",
    "    performance_metric.iloc[11,1]=str(format(mean11,'.3f'))+' ['+str(format(ci11[0],'.3f'))+'-'+str(format(ci11[1],'.3f'))+']'\n",
    "    ci12=stats.t.interval(0.95,99, loc=np.mean(ourmodelresult[:,12]), scale=stats.sem(ourmodelresult[:,12]))\n",
    "    mean12=np.mean(ourmodelresult[:,12])\n",
    "    performance_metric.iloc[12,0]='isotonic mae'\n",
    "    performance_metric.iloc[12,1]=str(format(mean12,'.3f'))+' ['+str(format(ci12[0],'.3f'))+'-'+str(format(ci12[1],'.3f'))+']'\n",
    "    performance_metric.to_csv(basefile+task+'_result/performance_metric'+task+'_'+modelname+'.csv',index=False,encoding='gbk')\n",
    "    roc_curve_onset6.columns=['mean_fpr','mean_tpr','tprs_upper','tprs_lower']\n",
    "    roc_curve_onset6.to_csv(basefile+task+'_result/roc_curve'+task+'_'+modelname+'.csv',index=False,encoding='gbk')\n",
    "    pr_curve_onset6.columns=['mean_recall','mean_precision','precision_upper','precision_lower']\n",
    "    pr_curve_onset6.to_csv(basefile+task+'_result/pr_curve'+task+'_'+modelname+'.csv',index=False,encoding='gbk')\n",
    "    re_calibration_curve_onset6.columns=['mean_prob_pred','mean_prob_trues','prob_true_upper','prob_true_lower']\n",
    "    re_calibration_curve_onset6.to_csv(basefile+task+'_result/re_calibration_curve'+task+'_'+modelname+'.csv',index=False,encoding='gbk')\n",
    "    calibration_curve_onset6.columns=['mean_prob_pred','mean_prob_trues','prob_true_upper','prob_true_lower']\n",
    "    calibration_curve_onset6.to_csv(basefile+task+'_result/calibration_curve'+task+'_'+modelname+'.csv',index=False,encoding='gbk')\n",
    "    explainer = shap.GradientExplainer(model,[a_train1,a_train2])\n",
    "    shap_values = explainer.shap_values([a_test1,a_test2])\n",
    "    shap_value=shap_values[0][0]\n",
    "    shap_value_time=shap_values[0][1]\n",
    "    np.save(basefile+task+'_result/shap_value'+task+'_'+modelname+'.npy',shap_value)\n",
    "    np.save(basefile+task+'_result/shap_value_time'+task+'_'+modelname+'.npy',shap_value_time)\n",
    "    np.save(basefile+task+'_result/a_test1'+task+'_'+modelname+'.npy',a_test1)\n",
    "    np.save(basefile+task+'_result/a_test2'+task+'_'+modelname+'.npy',a_test2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
